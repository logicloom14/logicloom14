def train_and_log_model(project_name, task_name, dataset_project, dataset_name, test_size, random_state, initial_lr, drop, epochs_drop, num_epochs, batch_size):
    # Initialize ClearML task
    import os
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    from tensorflow.keras.utils import to_categorical
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.layers import Flatten, Dense
    from tensorflow.keras.models import Model
    from tensorflow.keras.callbacks import LambdaCallback, LearningRateScheduler
    from tensorflow.keras.optimizers import Adam
    import math
    from clearml import Task, Dataset, OutputModel
    import matplotlib.pyplot as plt
    import pickle

    test_size=float(test_size)
    random_state=int(random_state)
    initial_lr=float(initial_lr)
    drop=float(drop)
    epochs_drop=int(epochs_drop)
    num_epochs=int(num_epochs)
    batch_size=int(batch_size)
    task = Task.init(
        project_name=project_name,
        task_name=task_name,
        task_type=Task.TaskTypes.training,
        auto_connect_frameworks="keras"
    )

    # Access the dataset
    dataset = Dataset.get(dataset_project=dataset_project, dataset_name=dataset_name)
    dataset_path = dataset.get_local_copy()

    data = np.load(os.path.join(dataset_path, "data_preprocessed.npy"))
    labels = np.load(os.path.join(dataset_path, "labels_preprocessed.npy"))

    # Encode labels
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(labels)
    labels = to_categorical(integer_encoded)

    # Create a new ClearML dataset for the labels
    labels_ds = Dataset.create(dataset_name="labels", dataset_project=project_name, parent_datasets=[dataset.id])
    label_encoder_pickle = os.path.join(os.getcwd(), "label_encoder.pkl")
    with open(label_encoder_pickle, "wb") as f:
        pickle.dump(label_encoder, f)
    labels_ds.add_files(label_encoder_pickle)
    labels_ds.upload()
    labels_ds.finalize()
    print(f"Labels dataset uploaded with ID: {labels_ds.id}")


    # Splitting the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=random_state)

    # Setup the model
    baseModel = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
    headModel = baseModel.output
    headModel = Flatten(name="flatten")(headModel)
    headModel = Dense(512, activation="relu")(headModel)
    headModel = Dense(len(np.unique(integer_encoded)), activation="softmax")(headModel)
    model = Model(inputs=baseModel.input, outputs=headModel)

    for layer in baseModel.layers:
        layer.trainable = False

    # Setup logging with ClearML
    logger = task.get_logger()

    # Lambda Callback for logging
    lambda_clbk = LambdaCallback(
        on_epoch_end=lambda epoch, logs: [
            logger.report_scalar("loss", "train", iteration=epoch, value=logs["loss"]),
            logger.report_scalar("accuracy", "train", iteration=epoch, value=logs["accuracy"]),
            logger.report_scalar("val_loss", "validation", iteration=epoch, value=logs["val_loss"]),
            logger.report_scalar("val_accuracy", "validation", iteration=epoch, value=logs["val_accuracy"]),
        ]
    )

    # Learning rate scheduler
    def lr_decay(epoch):
        lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))
        return lr

    lr_scheduler = LearningRateScheduler(lr_decay)

    # Compile and train the model
    model.compile(loss="categorical_crossentropy", optimizer=Adam(learning_rate=initial_lr), metrics=["accuracy"])
    H = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=num_epochs, batch_size=batch_size, callbacks=[lr_scheduler, lambda_clbk])
    # Plot training & validation accuracy values
    plt.plot(H.history['accuracy'])
    plt.plot(H.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    logger.report_matplotlib_figure(plt, "accu", "accu")
    plt.show()


    # Plot training & validation loss values
    plt.plot(H.history['loss'])
    plt.plot(H.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')

    logger.report_matplotlib_figure(plt, "accu", "accu")
    plt.show()
    # Prepare model file path
    model_file_path = os.path.join(os.getcwd(), "model.h5")

    # Save and upload the model to ClearML
    model.save(model_file_path)
    output_model = OutputModel(task=task)
    output_model.update_weights(model_file_path, upload_uri="https://files.clear.ml")
    output_model.publish()
    task.upload_artifact("trained_model", artifact_object=model_file_path)
    loss, accuracy = model.evaluate(X_test, y_test)
    logger.report_scalar("test_loss", "test", iteration=0, value=loss)
    logger.report_scalar("test_accuracy", "test", iteration=0, value=accuracy)
    # Complete the task
    task.close()
    return H

# Example usage with Google Colab, specify a directory within Colab's environment
# train_and_log_model(
#     project_name="AttendanceAI",
#     task_name="Model Training",
#     dataset_project="AttendanceAI",
#     dataset_name="processed_data",
#     test_size=0.25,
#     random_state=42,
#     initial_lr=1e-4,
#     drop=0.5,
#     epochs_drop=10.0,
#     num_epochs=20,
#     batch_size=32,
# )
